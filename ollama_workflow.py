#!/usr/bin/env python3
"""LangGraph workflow implementation with Ollama gpt-oss:20b model."""

from dotenv import load_dotenv
from src.main import main

# Load environment variables
load_dotenv()


# Configuration constants
class Config:
    # Ollama settings
    OLLAMA_MODEL = "gpt-oss:20b"
    OLLAMA_BASE_URL = "http://localhost:11434"
    OLLAMA_TEMPERATURE = 0.7

    # Search settings
    DEFAULT_SEARCH_DAYS_LIMIT = 60
    SEARCH_RESULT_LIMIT = 2000
    PARALLEL_SEARCH_LIMIT = 3
    SEARCH_TIMEOUT = 120
    INDIVIDUAL_RESULT_LIMIT = 1000

    # Time descriptions mapping
    TIME_DESCRIPTIONS = {
        1: "éå»1æ—¥",
        7: "éå»1é€±é–“",
        30: "éå»1ãƒ¶æœˆ",
        60: "éå»2ãƒ¶æœˆ",
        90: "éå»3ãƒ¶æœˆ",
        180: "éå»6ãƒ¶æœˆ",
        365: "éå»1å¹´",
    }

    # Recent search keywords
    RECENT_KEYWORDS = [
        "æœ€æ–°",
        "ç›´è¿‘",
        "æœ€è¿‘",
        "æ–°ã—ã„",
        "ä»Šæ—¥",
        "ä»Šé€±",
        "ä»Šæœˆ",
        "latest",
        "recent",
        "new",
        "current",
        "today",
        "this week",
        "this month",
        "ä»Šå¹´",
        "this year",
        "æœ€æ–°ç‰ˆ",
        "æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³",
        "current version",
        "latest version",
        "up to date",
        "ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ",
        "update",
    ]

    # Time-specific keywords mapping
    TIME_SPECIFIC_KEYWORDS = {
        "ä»Šæ—¥": 1,
        "today": 1,
        "ä»Šé€±": 7,
        "this week": 7,
        "ä»Šæœˆ": 30,
        "this month": 30,
        "ç›´è¿‘": 60,
        "æœ€è¿‘": 60,
        "recent": 60,
    }

    # Slack settings
    SLACK_MAX_RETRIES = 3
    SLACK_INITIAL_RETRY_DELAY = 2
    SLACK_CONTENT_LIMIT = 3000

    # Claude Code settings
    CLAUDE_MAX_TURNS = 1
    CLAUDE_WEBSEARCH_MAX_TURNS = 3

    # Threading settings
    MAX_WORKERS = 3


# Define the state structure for our workflow
class WorkflowState(TypedDict):
    messages: list[BaseMessage]
    iteration: int
    user_input: str
    original_user_input: str  # Store original question for iterations
    processed_output: str
    should_continue: bool
    search_results: str
    search_queries: list[str]  # Store generated search queries
    parallel_search_stats: dict  # Store parallel search statistics
    recent_search_mode: bool
    search_days_limit: int  # Store specific time limit for search filtering
    initial_output: str  # Store first AI output for comparison
    reviewed_output: str  # Store Claude Code reviewed output
    document_generated: bool  # Track document generation status
    document_content: str  # Store generated markdown content
    document_path: str  # Store path to generated document
    slack_notification_sent: bool  # Track Slack notification status


# Helper functions
def get_current_datetime_info() -> Dict[str, any]:
    """Get current datetime information in a consistent format."""
    current_datetime = datetime.datetime.now()
    return {
        "datetime": current_datetime,
        "year": current_datetime.year,
        "month": current_datetime.month,
        "day": current_datetime.day,
        "date_str": current_datetime.strftime("%Yå¹´%mæœˆ%dæ—¥"),
    }


def get_time_description(days: int) -> str:
    """Get human-readable time description for given days."""
    return Config.TIME_DESCRIPTIONS.get(days, f"éå»{days}æ—¥")


def build_psearch_command(
    query: str, recent_search_mode: bool, search_days_limit: int
) -> List[str]:
    """Build psearch command with appropriate filters."""
    psearch_cmd = ["psearch", "search", query[:100], "-n", "5", "-c", "--json"]

    if recent_search_mode:
        if search_days_limit <= 30:
            psearch_cmd.extend(["-r", "-s"])
        else:
            months = max(1, search_days_limit // 30)
            psearch_cmd.extend(["-r", "--months", str(months), "-s"])

    return psearch_cmd


def execute_psearch_with_progress(psearch_cmd: List[str]) -> Dict[str, any]:
    """Execute psearch command with real-time progress display."""
    import subprocess
    import sys

    start_time = time.time()

    try:
        process = subprocess.Popen(
            psearch_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True,
        )

        stdout_lines = []

        while True:
            output = process.stdout.readline()
            if output == "" and process.poll() is not None:
                break
            if output:
                print(f"ğŸ“¤ {output.rstrip()}")
                sys.stdout.flush()
                stdout_lines.append(output)

        stderr_output = process.stderr.read()
        return_code = process.wait()
        elapsed_time = time.time() - start_time

        return {
            "success": return_code == 0,
            "stdout": "".join(stdout_lines),
            "stderr": stderr_output,
            "elapsed_time": elapsed_time,
            "return_code": return_code,
        }

    except Exception as e:
        return {
            "success": False,
            "stdout": "",
            "stderr": str(e),
            "elapsed_time": time.time() - start_time,
            "return_code": -1,
            "error": e,
        }


def create_system_prompt(
    content: str, current_date_info: Dict[str, any], search_results: str, iteration: int
) -> str:
    """Create standardized system prompt for LLM calls."""
    return f"""
ã€é‡è¦ãªæŒ‡ç¤ºã€‘
- ã™ã¹ã¦ã®å›ç­”ã¯æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„
- ç¾åœ¨æ—¥æ™‚: {current_date_info["date_str"]} ({current_date_info["year"]}å¹´)
- æœ€æ–°æƒ…å ±ï¼ˆ{current_date_info["year"] - 1}å¹´ä»¥é™ï¼‰ã‚’å„ªå…ˆã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„

ã‚ãªãŸã¯LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®{iteration}å›ç›®ã®å‡¦ç†ã‚’è¡Œã†AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æœ€æ–°ã®æ¤œç´¢çµæœã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€æ­£ç¢ºã§æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦æ¤œç´¢çµæœã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–ã‚Šå…¥ã‚ŒãŸã€æ€æ…®æ·±ã„å›ç­”ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
ç°¡æ½”ã§ã‚ã‚ŠãªãŒã‚‰ã€æƒ…å ±é‡è±Šå¯Œãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚

ã€ç¾åœ¨ã®æ—¥æ™‚æƒ…å ±ã€‘
ç¾åœ¨ã¯{current_date_info["date_str"]}ï¼ˆ{current_date_info["year"]}å¹´ï¼‰ã§ã™ã€‚ã“ã®æ—¥æ™‚ã‚’è€ƒæ…®ã—ã¦ã€æœ€æ–°ã®æƒ…å ±ã‚’å„ªå…ˆã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›: {content}

æ¤œç´¢çµæœ (åˆ©ç”¨å¯èƒ½ãªå ´åˆ):
{search_results if search_results else "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“"}

ã€å›ç­”è¦ä»¶ã€‘
- ã™ã¹ã¦æ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„
- æ¤œç´¢çµæœã‚’æ´»ç”¨ã—ã¦ã€{current_date_info["year"]}å¹´æ™‚ç‚¹ã§ã®æœ€æ–°ã§æ­£ç¢ºãªæƒ…å ±ã‚’å«ã‚ã¦ãã ã•ã„
- å¤ã„æƒ…å ±ï¼ˆ{current_date_info["year"] - 2}å¹´ä»¥å‰ï¼‰ãŒã‚ã‚‹å ´åˆã¯ã€æœ€æ–°å‹•å‘ã‚‚ä½µè¨˜ã—ã¦ãã ã•ã„
- æŠ€è¡“çš„ãªå†…å®¹ã®å ´åˆã¯ã€æœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚„ä»•æ§˜å¤‰æ›´ã‚‚è€ƒæ…®ã—ã¦ãã ã•ã„
"""


def search_node(state: WorkflowState) -> WorkflowState:
    """Search for relevant information using psearch with real-time output and enhanced date filtering."""
    user_input = state.get("user_input", "")
    recent_search_mode = state.get("recent_search_mode", False)
    search_days_limit = state.get("search_days_limit", Config.DEFAULT_SEARCH_DAYS_LIMIT)

    if not user_input:
        return {**state, "search_results": ""}

    current_date_info = get_current_datetime_info()

    search_mode_text = ""
    if recent_search_mode:
        time_desc = get_time_description(search_days_limit)
        search_mode_text = f" ({time_desc}ã®æƒ…å ±ã«é™å®š)"

    print(f"ğŸ” Searching for information about: {user_input}{search_mode_text}")
    if recent_search_mode:
        time_desc = get_time_description(search_days_limit)
        print(
            f"ğŸ“… Date filtering active: {time_desc} ({current_date_info['date_str']}åŸºæº–)"
        )
    print("ğŸ“Š Progress visualization:")
    print("-" * 40)

    try:
        # Build and execute search command
        psearch_cmd = build_psearch_command(
            user_input, recent_search_mode, search_days_limit
        )

        # Show filtering info
        if recent_search_mode:
            time_desc = get_time_description(search_days_limit)
            if search_days_limit <= 30:
                print(f"ğŸ“… Filtering results: {time_desc}ä»¥å†…, æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆ")
            else:
                months = max(1, search_days_limit // 30)
                print(f"ğŸ“… Filtering results: éå»{months}ãƒ¶æœˆä»¥å†…, æ—¥ä»˜é †ã‚½ãƒ¼ãƒˆ")

        # Execute search with progress display
        result = execute_psearch_with_progress(psearch_cmd)

        print("-" * 40)
        print(f"â±ï¸ Search completed in {result['elapsed_time']:.2f} seconds")

        if result["success"]:
            print("âœ… Search completed successfully")
            search_output = result["stdout"]
            result_count = (
                len(search_output.split("---")) - 1
                if "---" in search_output
                else "some"
            )
            print(f"ğŸ“„ Found {result_count} results")

            # Summarize search results for LLM processing
            search_summary = f"Search results for '{user_input}':\n\n{search_output[: Config.SEARCH_RESULT_LIMIT]}..."
            return {**state, "search_results": search_summary}
        else:
            print(f"âš ï¸ Search failed with return code {result['return_code']}")
            print(f"Error: {result['stderr']}")
            return {**state, "search_results": f"Search failed: {result['stderr']}"}

    except FileNotFoundError:
        print("âŒ psearch command not found")
        return {**state, "search_results": "psearch command not available"}
    except Exception as e:
        print(f"âŒ Search error: {e}")
        return {**state, "search_results": f"Search error: {str(e)}"}


def detect_recent_search_mode(
    user_input: str, current_date_info: Dict[str, any]
) -> tuple[bool, int]:
    """Detect if recent search mode should be activated and determine time limit."""
    # Enhanced keywords including dynamic current year
    recent_keywords = Config.RECENT_KEYWORDS + [
        f"{current_date_info['year']}å¹´",
        f"{current_date_info['year'] - 1}å¹´",
    ]

    recent_search_mode = any(keyword in user_input for keyword in recent_keywords)

    # Determine specific time range
    search_days_limit = Config.DEFAULT_SEARCH_DAYS_LIMIT
    for keyword, days in Config.TIME_SPECIFIC_KEYWORDS.items():
        if keyword in user_input:
            search_days_limit = min(search_days_limit, days)
            break

    if recent_search_mode:
        time_description = get_time_description(search_days_limit)
        print(
            f"ğŸ” Recent information keywords detected - search will be limited to {time_description}"
        )
        filter_year = current_date_info["year"] - (1 if search_days_limit > 30 else 0)
        print(
            f"ğŸ“… Current date: {current_date_info['date_str']} - filtering for content from {filter_year}å¹´ä»¥é™"
        )

    return recent_search_mode, search_days_limit


def input_node(state: WorkflowState) -> WorkflowState:
    """Process initial user input and detect recent search keywords."""
    user_input = state.get("user_input", "")
    messages = state.get("messages", [])

    # Add user message to conversation
    if user_input:
        messages.append(HumanMessage(content=user_input))

    current_date_info = get_current_datetime_info()
    recent_search_mode, search_days_limit = detect_recent_search_mode(
        user_input, current_date_info
    )

    return {
        **state,
        "messages": messages,
        "iteration": state.get("iteration", 0) + 1,
        "recent_search_mode": recent_search_mode,
        "search_days_limit": search_days_limit,
        "original_user_input": state.get("original_user_input", user_input),
    }


def create_ollama_llm():
    """Create and return configured Ollama LLM instance."""
    return ChatOllama(
        model=Config.OLLAMA_MODEL,
        base_url=Config.OLLAMA_BASE_URL,
        temperature=Config.OLLAMA_TEMPERATURE,
    )


def handle_ollama_fallback(
    messages: List[BaseMessage], iteration: int
) -> Dict[str, any]:
    """Handle Ollama fallback when service is unavailable."""
    if messages and isinstance(messages[-1], HumanMessage):
        content = messages[-1].content
        fallback_response = (
            f"Processing iteration {iteration}: {content} (Ollama unavailable)"
        )
        messages.append(AIMessage(content=fallback_response))

        return {
            "messages": messages,
            "processed_output": fallback_response,
        }
    return {"messages": messages}


def processing_node(state: WorkflowState) -> WorkflowState:
    """Process the user input using Ollama gpt-oss:20b model with search results."""
    messages = state["messages"]
    iteration = state["iteration"]
    search_results = state.get("search_results", "")

    if not messages:
        return state

    print(f"ğŸ¤– Processing iteration {iteration} with Ollama {Config.OLLAMA_MODEL}...")

    try:
        llm = create_ollama_llm()
        last_message = messages[-1]

        if isinstance(last_message, HumanMessage):
            content = last_message.content
            current_date_info = get_current_datetime_info()

            # Create system prompt using helper function
            system_prompt = create_system_prompt(
                content, current_date_info, search_results, iteration
            )

            # Get response from Ollama
            response = llm.invoke([HumanMessage(content=system_prompt)])
            ai_response = (
                response.content if response.content else "å¿œç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
            )
            messages.append(AIMessage(content=ai_response))

            print("âœ… LLM Full Response:")
            print("-" * 60)
            print(ai_response)
            print("-" * 60)

            return {
                **state,
                "messages": messages,
                "processed_output": ai_response,
                "initial_output": ai_response,
            }

    except Exception as e:
        print(f"âŒ Error calling Ollama: {e}")
        print("ğŸ”„ Falling back to simple response generation...")

        fallback_result = handle_ollama_fallback(messages, iteration)
        return {**state, **fallback_result}

    return state


def create_claude_code_options(
    system_prompt: str, max_turns: int = None, allowed_tools: List[str] = None
):
    """Create standardized Claude Code options."""
    from claude_code_sdk import ClaudeCodeOptions

    options = ClaudeCodeOptions(
        system_prompt=system_prompt,
        max_turns=max_turns or Config.CLAUDE_MAX_TURNS,
        allowed_tools=allowed_tools or ["WebSearch"],
    )

    # Add context7 MCP server if needed
    if "context7" in system_prompt.lower() or not allowed_tools:
        options.mcp_servers = {
            "context7": {"command": "npx", "args": ["-y", "@context7/server"]}
        }

    return options


def create_review_system_prompt(
    processed_output: str, original_question: str, current_date_info: Dict[str, any]
) -> str:
    """Create detailed system prompt for Claude Code review."""
    return f"""ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ã®æ ¡æ­£ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚

ä¸Šè¨˜ã®å›ç­”å†…å®¹ã«ã¤ã„ã¦ã€è©³ç´°ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã„ã€ãã®å¾Œã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’åæ˜ ã—ãŸä¿®æ­£ç‰ˆã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

ã€ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®å›ç­”å†…å®¹ã€‘
{processed_output}

ã€å…ƒã®è³ªå•ã€‘
{original_question}

ã€é‡è¦ãªæŒ‡ç¤ºã€‘
- å¿…ãšæ—¥æœ¬èªã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ä¿®æ­£ç‰ˆã‚’ä½œæˆã—ã¦ãã ã•ã„
- ã™ã¹ã¦ã®å‡ºåŠ›ã¯æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ 
- WebSearchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã‚‚ã€çµæœã¯æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„
- ç¾åœ¨æ—¥æ™‚: {current_date_info["date_str"]} ({current_date_info["year"]}å¹´)
- æœ€æ–°æƒ…å ±ï¼ˆ{current_date_info["year"] - 1}å¹´ä»¥é™ï¼‰ã‚’å„ªå…ˆã—ã¦å‚ç…§ã—ã¦ãã ã•ã„

ã€å€‹åˆ¥ã®æŒ‡ç¤ºã€‘
- ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡ã®æ–‡ç« ã‚’ã‚‚ã¨ã«ã€å¿…ãšã€Œè©³ç´°ãªãƒ¬ãƒ“ãƒ¥ãƒ¼ã€ã¨ã€Œä¿®æ­£ç‰ˆã€ã®ä¸¡æ–¹ã‚’ä½œæˆã—ã¦ãã ã•ã„
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§æŒ‡æ‘˜ã—ãŸå†…å®¹ã¯ä¿®æ­£ç‰ˆã«ã™ã¹ã¦åæ˜ ã—ã¦ãã ã•ã„
- ã™ã¹ã¦ã®å‡ºåŠ›ã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå†…å®¹ã¯æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„

---

## ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒã‚¤ãƒ³ãƒˆ

### 1. ä¸€èˆ¬çš„ãªãƒã‚¤ãƒ³ãƒˆ
1. äº‹å®Ÿã®æ­£ç¢ºæ€§ï¼ˆé–“é•ã„ã‚„å¤ã„æƒ…å ±ãŒãªã„ã‹ã€ç‰¹ã«{current_date_info["year"] - 1}å¹´ä»¥é™ã®æœ€æ–°æƒ…å ±ã¨ã®æ•´åˆæ€§ï¼‰
2. è«–ç†çš„ãªä¸€è²«æ€§ï¼ˆçŸ›ç›¾ãŒãªã„ã‹ï¼‰
3. å®Œå…¨æ€§ï¼ˆé‡è¦ãªæƒ…å ±ãŒæŠœã‘ã¦ã„ãªã„ã‹ï¼‰
4. ã‚ã‹ã‚Šã‚„ã™ã•ï¼ˆèª¬æ˜ãŒæ˜ç¢ºã‹ï¼‰
5. æœ€æ–°æ€§ï¼ˆ{current_date_info["year"]}å¹´ã®æœ€æ–°æƒ…å ±ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ï¼‰

### 2. æŠ€è¡“çš„è³ªå•ã®å ´åˆã®è¿½åŠ ãƒã‚¤ãƒ³ãƒˆ
- æŠ€è¡“çš„æ­£ç¢ºæ€§ï¼ˆã‚³ãƒ¼ãƒ‰æ§‹æ–‡ã€APIã®ä½¿ç”¨æ–¹æ³•ã€{current_date_info["year"]}å¹´æ™‚ç‚¹ã§ã®æœ€æ–°ä»•æ§˜ï¼‰
- ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹æº–æ‹ ï¼ˆæ¥­ç•Œæ¨™æº–ã«å¾“ã£ã¦ã„ã‚‹ã‹ï¼‰
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ï¼ˆãƒªã‚¹ã‚¯ã‚„å•é¡ŒãŒãªã„ã‹ï¼‰
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ï¼ˆåŠ¹ç‡çš„ã§æœ€é©åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
- å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ã®æ•´åˆæ€§ï¼ˆ{current_date_info["year"]}å¹´ã®æœ€æ–°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«åŸºã¥ãã‹ï¼‰
- å®Ÿè£…ä¸Šã®æ³¨æ„ç‚¹ã‚„è½ã¨ã—ç©´ï¼ˆæœ€æ–°ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®å¤‰æ›´ç‚¹ã‚’å«ã‚€ï¼‰

### 3. æœ€æ–°æƒ…å ±ç¢ºèª
- WebSearchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦{current_date_info["year"]}å¹´ã®æœ€æ–°æƒ…å ±ã‚’ç¢ºèªã™ã‚‹ã“ã¨
- å¤ã„æƒ…å ±ï¼ˆ{current_date_info["year"] - 2}å¹´ä»¥å‰ï¼‰ã¯æœ€æ–°æƒ…å ±ã§è£œå®Œã™ã‚‹ã“ã¨
- ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ã‚„APIå¤‰æ›´ãªã©æœ€æ–°å‹•å‘ã‚’åæ˜ ã™ã‚‹ã“ã¨
- WebSearchã®çµæœã¯å¿…ãšæ—¥æœ¬èªã§ã¾ã¨ã‚ã‚‹ã“ã¨

---

## å‡ºåŠ›å½¢å¼ï¼ˆå¿…ãšæ—¥æœ¬èªã§è¨˜è¿°ï¼‰

1. **è©³ç´°ãªãƒ¬ãƒ“ãƒ¥ãƒ¼**ï¼ˆç®‡æ¡æ›¸ããƒ»å…·ä½“çš„ã«ã€æ—¥æœ¬èªã§çœç•¥ã›ãšï¼‰
2. **ä¿®æ­£ç‰ˆæ–‡ç« **ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å†…å®¹ã‚’å®Œå…¨ã«åæ˜ ã—ãŸä¿®æ­£ç‰ˆã€æ—¥æœ¬èªã§å®Œå…¨ã«æ›¸ãï¼‰
3. **ä¿®æ­£ç‚¹ã®èª¬æ˜**ï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼å†…å®¹ã«æ²¿ã£ã¦ä½•ã‚’ã©ã†ä¿®æ­£ã—ãŸã‹ã€æ—¥æœ¬èªã§è©³ç´°ã«ï¼‰
4. å•é¡ŒãŒãªã‘ã‚Œã°ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†ï¼šå•é¡Œãªã—ï¼ˆ{current_date_info["date_str"]}æ™‚ç‚¹ï¼‰ã€ã¨è¨˜è¿°

---

ã€æœ€é‡è¦ã€‘
- ã™ã¹ã¦ã®å‡ºåŠ›ã¯æ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„
- WebSearchã®çµæœã‚„å¼•ç”¨ã‚‚æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„
- è‹±èªã®ãƒ†ã‚­ã‚¹ãƒˆã¯å«ã‚ãªã„ã§ãã ã•ã„
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨ä¿®æ­£ç‰ˆã¯å¿…ãšã‚»ãƒƒãƒˆã§æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„
- æŠ€è¡“çš„ãªæƒ…å ±ã¯çœç•¥ã›ãšã€æœ€æ–°æƒ…å ±ï¼ˆ{current_date_info["year"]}å¹´ï¼‰ã«åŸºã¥ãæ›´æ–°ç‚¹ã‚’æ˜ç¤ºã—ã¦ãã ã•ã„"""


async def execute_claude_code_query(prompt: str, options) -> str:
    """Execute Claude Code query and return content."""
    from claude_code_sdk import query

    content = ""
    message_count = 0

    try:
        async for message in query(prompt=prompt, options=options):
            message_count += 1
            print(f"ğŸ“¨ Received message #{message_count} from Claude Code SDK")

            if hasattr(message, "content"):
                if isinstance(message.content, list):
                    for i, block in enumerate(message.content):
                        print(
                            f"ğŸ“„ Processing content block #{i + 1} - Type: {type(block).__name__}"
                        )

                        try:
                            from claude_code_sdk.types import (
                                TextBlock,
                                ToolUseBlock,
                                ToolResultBlock,
                            )

                            if isinstance(block, TextBlock):
                                content += block.text
                            elif isinstance(block, ToolUseBlock):
                                tool_name = getattr(block, "name", "unknown")
                                print(f"ğŸ”§ ToolUseBlock - Tool: {tool_name}")
                                content += f"\n[ãƒ„ãƒ¼ãƒ«ä½¿ç”¨: {tool_name}]\n"
                            elif isinstance(block, ToolResultBlock):
                                tool_result = str(
                                    getattr(block, "content", "no result")
                                )
                                print(
                                    f"ğŸ“¤ ToolResultBlock - Result length: {len(tool_result)} characters"
                                )
                                content += f"\n[ãƒ„ãƒ¼ãƒ«çµæœ: {tool_result}]\n"
                            else:
                                if hasattr(block, "text"):
                                    content += block.text

                        except ImportError:
                            print(
                                "âš ï¸ Could not import specific block types, using fallback"
                            )
                            if hasattr(block, "text"):
                                content += block.text
                else:
                    content += str(message.content)

    except Exception as query_error:
        print(f"âŒ Error during Claude Code SDK query: {query_error}")
        raise query_error

    print(
        f"âœ… Query completed. Total messages: {message_count}, Content length: {len(content)}"
    )
    return content


def review_node(state: WorkflowState) -> WorkflowState:
    """Use Claude Code SDK to review and correct the final output."""
    processed_output = state.get("processed_output", "")
    original_question = state.get("original_user_input", "")

    if not processed_output:
        print("âš ï¸ No output to review")
        return {**state, "reviewed_output": ""}

    print("ğŸ” Reviewing output with Claude Code SDK...")
    print("ğŸ“‹ Starting Claude Code SDK review process...")

    try:
        print("ğŸ“¦ Importing Claude Code SDK...")
        print("âœ… Claude Code SDK imported successfully")

        print("âš™ï¸ Configuring Claude Code options with context7 MCP...")

        current_date_info = get_current_datetime_info()
        detailed_system_prompt = create_review_system_prompt(
            processed_output, original_question, current_date_info
        )

        options = create_claude_code_options(detailed_system_prompt)
        print("âœ… Claude Code options configured with context7 MCP server")
        print(
            f"ğŸ”§ MCP servers configured: {list(options.mcp_servers.keys()) if options.mcp_servers else 'None'}"
        )
        print(f"ğŸ› ï¸ Allowed tools: {options.allowed_tools}")

        print("ğŸ”„ Starting async query to Claude Code SDK...")

        simple_prompt = "ä¸Šè¨˜ã®å›ç­”å†…å®¹ã‚’æ—¥æœ¬èªã§è©³ç´°ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚ã™ã¹ã¦ã®å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
        print(f"ğŸ“ Prompt length: {len(simple_prompt)} characters")

        import asyncio

        print("ğŸš€ Executing async query...")
        reviewed_content = asyncio.run(
            execute_claude_code_query(simple_prompt, options)
        )
        print("âœ… Async query completed successfully")

        print("âœ… Review completed with Claude Code SDK")
        print("-" * 60)
        print(reviewed_content)
        print("-" * 60)

        return {**state, "reviewed_output": reviewed_content}

    except ImportError as import_error:
        print(f"âŒ Claude Code SDK not available: {import_error}")
        return handle_claude_code_error(
            "SDKåˆ©ç”¨ä¸å¯", processed_output, import_error, state
        )
    except Exception as e:
        print(f"âŒ Error during review: {e}")
        return handle_claude_code_error(
            "ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ", processed_output, e, state
        )


def handle_claude_code_error(
    error_type: str, processed_output: str, error: Exception, state: WorkflowState
) -> WorkflowState:
    """Handle Claude Code SDK errors consistently."""
    import traceback

    print(f"ğŸ” Error type: {type(error)}")
    traceback.print_exc()

    error_message = f"{error_type}: {error}\n\nå…ƒã®å›ç­”:\n{processed_output}"
    return {**state, "reviewed_output": error_message}


def documentation_node(state: WorkflowState) -> WorkflowState:
    """Generate markdown documentation comparing initial and final outputs."""
    original_question = state.get("original_user_input", "")
    initial_output = state.get("initial_output", "")
    reviewed_output = state.get("reviewed_output", "")
    search_results = state.get("search_results", "")

    print("ğŸ“ Generating documentation...")

    try:
        # Create docs directory if it doesn't exist
        docs_dir = Path.home() / "workspace" / "Docs"
        docs_dir.mkdir(parents=True, exist_ok=True)

        # Create a descriptive title from the original question
        question_summary = (
            original_question[:30]
            .replace("/", "")
            .replace("\\", "")
            .replace(":", "ï¼š")
            .replace("?", "ï¼Ÿ")
            .replace("*", "")
            .replace("<", "")
            .replace(">", "")
            .replace("|", "")
        )
        if len(original_question) > 30:
            question_summary += "..."

        filename = f"{question_summary}_åˆ†æçµæœ.md"
        file_path = docs_dir / filename

        # Extract final corrected version if available
        final_corrected_version = ""
        if reviewed_output:
            try:
                # Try to extract corrected content from review output
                # Look for patterns like "ä¿®æ­£ç‰ˆ:" or actual corrected text sections
                import re

                # Look for corrected text in the review output with more comprehensive patterns
                corrected_patterns = [
                    r"ä¿®æ­£ç‰ˆ[ï¼š:]\s*\n(.+?)(?=\n\n##|\n\n---|\Z)",
                    r"ä¿®æ­£[ï¼š:]\s*\n(.+?)(?=\n\n##|\n\n---|\Z)",
                    r"æ”¹å–„ç‰ˆ[ï¼š:]\s*\n(.+?)(?=\n\n##|\n\n---|\Z)",
                    r"ä»¥ä¸‹ãŒä¿®æ­£ç‰ˆã§ã™[ï¼š:]?\s*\n(.+?)(?=\n\n##|\n\n---|\Z)",
                    r"ä¿®æ­£å¾Œ[ï¼š:]?\s*\n(.+?)(?=\n\n##|\n\n---|\Z)",
                ]

                for pattern in corrected_patterns:
                    match = re.search(
                        pattern, reviewed_output, re.DOTALL | re.MULTILINE
                    )
                    if match:
                        final_corrected_version = match.group(1).strip()
                        print(
                            f"âœ… Extracted corrected version using pattern: {pattern[:20]}..."
                        )
                        break

                # If no explicit corrected version found, check if the review contains substantial corrections
                # Look for structured corrections or improvements
                if not final_corrected_version:
                    # Check for markdown-style corrections or improvements
                    improvement_patterns = [
                        r"## ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ.*?## ä¿®æ­£å†…å®¹.*?\n(.+?)(?=\n## |$)",
                        r"### ä¿®æ­£å†…å®¹\s*\n(.+?)(?=\n### |$)",
                        r"**ä¿®æ­£ç‰ˆ**\s*\n(.+?)(?=\n**|$)",
                        r"\*\*ä¿®æ­£ç‰ˆ\*\*\s*\n(.+?)(?=\n\*\*|$)",
                    ]

                    for pattern in improvement_patterns:
                        match = re.search(
                            pattern, reviewed_output, re.DOTALL | re.MULTILINE
                        )
                        if match:
                            final_corrected_version = match.group(1).strip()
                            print("âœ… Extracted improvement section using pattern")
                            break

                # If still no corrected version, check if the review contains substantial content that looks like a correction
                if (
                    not final_corrected_version
                    and "ä¿®æ­£" in reviewed_output
                    and len(reviewed_output) > 1000
                ):
                    # Check if the review output seems to contain a complete corrected version
                    # Look for technical content or structured information
                    if any(
                        keyword in reviewed_output
                        for keyword in [
                            "Linear",
                            "GitHub",
                            "æ©Ÿèƒ½",
                            "å®Ÿè£…",
                            "è¨­å®š",
                            "æ‰‹é †",
                        ]
                    ):
                        print(
                            "âœ… Using complete review output as it contains substantial technical corrections"
                        )
                        final_corrected_version = reviewed_output

            except Exception as e:
                print(f"âš ï¸ Could not extract corrected version: {e}")

        # Generate markdown content
        markdown_content = f"""# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œçµæœ

## å®Ÿè¡Œæƒ…å ±
- **å®Ÿè¡Œæ—¥æ™‚**: {datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}
- **è³ªå•**: {original_question}
- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: {state.get("iteration", 0)}

## å…ƒã®è³ªå•
```
{original_question}
```

## æ¤œç´¢çµæœã®æ¦‚è¦
```
{search_results[:500] if search_results else "æ¤œç´¢çµæœãªã—"}...
```

## 1. åˆå›AIå›ç­”ï¼ˆOllama gpt-oss:20bï¼‰
{initial_output if initial_output else "åˆå›å›ç­”ãªã—"}

## 2. Claude Codeãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»ä¿®æ­£çµæœ
{reviewed_output if reviewed_output else "ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœãªã—"}

{
            f'''## 3. æœ€çµ‚ä¿®æ­£ç‰ˆ

ä»¥ä¸‹ã¯Claude Codeãƒ¬ãƒ“ãƒ¥ãƒ¼ã«åŸºã¥ãä¿®æ­£ç‰ˆã§ã™ï¼š

{final_corrected_version}

### ä¿®æ­£ã®è©³ç´°èª¬æ˜
ä¸Šè¨˜ã®ä¿®æ­£ç‰ˆã¯å…ƒã®å›ç­”ã«å¯¾ã™ã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§æŒ‡æ‘˜ã•ã‚ŒãŸä»¥ä¸‹ã®æ”¹å–„ç‚¹ã‚’åæ˜ ã—ã¦ã„ã¾ã™ï¼š
- æŠ€è¡“çš„æ­£ç¢ºæ€§ã®å‘ä¸Š
- æœ€æ–°æƒ…å ±ã®è¿½åŠ 
- è«–ç†çš„ä¸€è²«æ€§ã®æ”¹å–„
- å®Œå…¨æ€§ã®å‘ä¸Š
'''
            if final_corrected_version and final_corrected_version != reviewed_output
            else ""
        }


---
*ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ LangGraph + Claude Code SDK ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""

        # Write to file
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)

        print(f"âœ… Documentation generated: {file_path}")

        return {
            **state,
            "document_generated": True,
            "document_content": markdown_content,
            "document_path": str(file_path),
        }

    except Exception as e:
        print(f"âŒ Error generating documentation: {e}")
        return {
            **state,
            "document_generated": False,
            "document_content": "",
            "document_path": "",
        }


def generate_search_queries(state: WorkflowState) -> WorkflowState:
    """Generate exactly 3 diverse search queries using Claude Code agent."""
    user_input = state.get("user_input", "")

    if not user_input:
        return {**state, "search_queries": []}

    print(f"ğŸ§  Generating 3 search queries using Claude Code agent for: {user_input}")

    try:
        # Use Claude Code SDK to generate diverse search queries
        print("ğŸ“¦ Using Claude Code agent for query generation...")
        from claude_code_sdk import query as claude_query, ClaudeCodeOptions

        # Configure options for Claude Code
        query_generation_prompt = f"""ã‚ãªãŸã¯æ¤œç´¢æˆ¦ç•¥ã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®è³ªå•ã«å¯¾ã—ã¦ã€åŠ¹æœçš„ãªæ¤œç´¢ã‚’è¡Œã†ãŸã‚ã«3ã¤ã®ç•°ãªã‚‹è§’åº¦ã‹ã‚‰ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

å…ƒã®è³ªå•: {user_input}

ä»¥ä¸‹ã®è¦ä»¶ã«å¾“ã£ã¦ã€ã¡ã‚‡ã†ã©3ã¤ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

1. **åŸºæœ¬æ¦‚å¿µã‚¯ã‚¨ãƒª**: æ ¸ã¨ãªã‚‹æ¦‚å¿µã‚„å®šç¾©ã‚’æ¢ã™ã‚¯ã‚¨ãƒª
2. **æœ€æ–°æƒ…å ±ã‚¯ã‚¨ãƒª**: æœ€æ–°ã®å‹•å‘ã‚„æ›´æ–°æƒ…å ±ã‚’æ¢ã™ã‚¯ã‚¨ãƒª  
3. **å®Ÿè·µçš„ã‚¯ã‚¨ãƒª**: å®Ÿè£…ä¾‹ã‚„å…·ä½“çš„ãªä½¿ç”¨ä¾‹ã‚’æ¢ã™ã‚¯ã‚¨ãƒª

å„ã‚¯ã‚¨ãƒªã¯50æ–‡å­—ä»¥å†…ã§ã€ç°¡æ½”ã‹ã¤åŠ¹æœçš„ã«ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã®å½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
ã‚¯ã‚¨ãƒª1: [æ¤œç´¢ã‚¯ã‚¨ãƒª1]
ã‚¯ã‚¨ãƒª2: [æ¤œç´¢ã‚¯ã‚¨ãƒª2]
ã‚¯ã‚¨ãƒª3: [æ¤œç´¢ã‚¯ã‚¨ãƒª3]"""

        options = ClaudeCodeOptions(
            system_prompt="ã‚ãªãŸã¯æ¤œç´¢æˆ¦ç•¥ã®å°‚é–€å®¶ã§ã™ã€‚ä¸ãˆã‚‰ã‚ŒãŸè³ªå•ã«å¯¾ã—ã¦ã€3ã¤ã®ç•°ãªã‚‹è§’åº¦ã‹ã‚‰åŠ¹æœçš„ãªæ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚",
            max_turns=1,
        )

        async def get_queries():
            content = ""
            async for message in claude_query(
                prompt=query_generation_prompt, options=options
            ):
                if hasattr(message, "content"):
                    if isinstance(message.content, list):
                        for block in message.content:
                            if hasattr(block, "text"):
                                content += block.text
                    else:
                        content += str(message.content)
            return content

        # Run async function
        import asyncio

        query_response = asyncio.run(get_queries())

        # Extract queries from the response
        query_pattern = r"ã‚¯ã‚¨ãƒª\d+:\s*(.+)"
        matches = re.findall(query_pattern, query_response)

        # Clean up and limit to exactly 3 queries
        queries = [q.strip() for q in matches if q.strip()]
        queries = queries[:3]  # Limit to exactly 3 queries

        # If we got less than 3 queries, create fallback queries
        if len(queries) < 3:
            print("âš ï¸ Claude Code agent returned fewer than 3 queries, using fallback")
            queries = [
                user_input,  # Basic query
                f"{user_input} æœ€æ–°",  # Latest info query
                f"{user_input} å®Ÿè£…",  # Implementation query
            ][:3]

        # Ensure we have exactly 3 queries
        queries = queries[:3]

        print(
            f"âœ… Generated exactly {len(queries)} search queries using Claude Code agent:"
        )
        for i, q in enumerate(queries, 1):
            print(f"  {i}. {q}")

        return {**state, "search_queries": queries}

    except ImportError:
        print("âŒ Claude Code SDK not available, falling back to rule-based generation")
        # Fallback: create exactly 3 basic queries from user input
        fallback_queries = [
            user_input,  # Basic query
            f"{user_input} æœ€æ–°",  # Latest info query
            f"{user_input} å®Ÿè£…æ–¹æ³•",  # Implementation query
        ]
        return {**state, "search_queries": fallback_queries}

    except Exception as e:
        print(f"âŒ Error with Claude Code agent: {e}")
        # Fallback: create exactly 3 basic queries from user input
        fallback_queries = [
            user_input,  # Basic query
            f"{user_input} æœ€æ–°",  # Latest info query
            f"{user_input} å®Ÿè£…æ–¹æ³•",  # Implementation query
        ]
        return {**state, "search_queries": fallback_queries}


def execute_single_search(
    query_info: tuple, recent_search_mode: bool, search_days_limit: int
) -> Dict[str, any]:
    """Execute a single search with proper error handling."""
    import subprocess

    query_index, query = query_info
    print(f"ğŸ” Search {query_index + 1}: {query}")

    try:
        # Build psearch command using existing helper
        psearch_cmd = build_psearch_command(
            query, recent_search_mode, search_days_limit
        )
        # Override some settings for parallel search
        psearch_cmd[4] = "3"  # Change -n to 3 for parallel searches

        # Execute search with timeout
        start_time = time.time()
        result = subprocess.run(
            psearch_cmd, capture_output=True, text=True, timeout=Config.SEARCH_TIMEOUT
        )

        elapsed_time = time.time() - start_time

        if result.returncode == 0:
            print(f"âœ… Search {query_index + 1} completed in {elapsed_time:.2f}s")
            return {
                "query": query,
                "results": result.stdout,
                "success": True,
                "elapsed_time": elapsed_time,
            }
        else:
            print(f"âŒ Search {query_index + 1} failed: {result.stderr}")
            return {
                "query": query,
                "results": f"Search failed: {result.stderr}",
                "success": False,
                "elapsed_time": elapsed_time,
            }

    except subprocess.TimeoutExpired:
        print(f"â° Search {query_index + 1} timed out")
        return {
            "query": query,
            "results": "Search timed out",
            "success": False,
            "elapsed_time": Config.SEARCH_TIMEOUT,
        }
    except Exception as e:
        print(f"âŒ Search {query_index + 1} error: {e}")
        return {
            "query": query,
            "results": f"Search error: {str(e)}",
            "success": False,
            "elapsed_time": 0,
        }


def execute_websearch_fallback(search_queries: List[str]) -> str:
    """Execute WebSearch fallback when all parallel searches fail."""
    try:
        main_query = search_queries[0] if search_queries else ""
        print(f"ğŸŒ Using WebSearch fallback for: {main_query}")

        websearch_prompt = f"""ä»¥ä¸‹ã®ã‚¯ã‚¨ãƒªã«ã¤ã„ã¦ã€WebSearchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã—ã€æ¤œç´¢çµæœã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ï¼š

ã‚¯ã‚¨ãƒª: {main_query}

è¦æ±‚äº‹é …:
- æœ€æ–°ã®æƒ…å ±ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„
- è¤‡æ•°ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’åé›†ã—ã¦ãã ã•ã„
- æ¤œç´¢çµæœã‚’æ—¥æœ¬èªã§ã¾ã¨ã‚ã¦ãã ã•ã„
- ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‚’å„ªå…ˆã—ã¦ãã ã•ã„"""

        options = create_claude_code_options(
            "ã‚ãªãŸã¯æƒ…å ±æ¤œç´¢ã®å°‚é–€å®¶ã§ã™ã€‚WebSearchãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸã‚¯ã‚¨ãƒªã«é–¢ã™ã‚‹æœ€æ–°ã§æ­£ç¢ºãªæƒ…å ±ã‚’æ¤œç´¢ã—ã€çµæœã‚’ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚",
            max_turns=Config.CLAUDE_WEBSEARCH_MAX_TURNS,
        )

        import asyncio

        return asyncio.run(execute_claude_code_query(websearch_prompt, options))

    except Exception as e:
        print(f"âŒ WebSearch fallback failed: {e}")
        return f"WebSearch fallback error: {str(e)}"


def format_parallel_search_results(
    search_results: List[Dict[str, any]], total_elapsed_time: float
) -> str:
    """Format parallel search results into a readable summary."""
    successful_searches = [r for r in search_results if r["success"]]

    combined_results = f"Parallel Search Results ({len(successful_searches)}/{len(search_results)} successful):\n\n"

    for i, result in enumerate(search_results, 1):
        status = "âœ…" if result["success"] else "âŒ"
        combined_results += f"{status} Search {i}: {result['query']}\n"
        combined_results += f"Time: {result['elapsed_time']:.2f}s\n"

        if result["success"] and result["results"]:
            limited_results = result["results"][: Config.INDIVIDUAL_RESULT_LIMIT]
            if len(result["results"]) > Config.INDIVIDUAL_RESULT_LIMIT:
                limited_results += "..."
            combined_results += f"Results:\n{limited_results}\n"
        else:
            combined_results += f"Error: {result['results']}\n"
        combined_results += "-" * 50 + "\n\n"

    return combined_results


def parallel_search_node(state: WorkflowState) -> WorkflowState:
    """Execute multiple searches in parallel using the generated queries."""
    search_queries = state.get("search_queries", [])
    recent_search_mode = state.get("recent_search_mode", False)
    search_days_limit = state.get("search_days_limit", Config.DEFAULT_SEARCH_DAYS_LIMIT)

    if not search_queries:
        print("âš ï¸ No search queries available")
        return {**state, "search_results": ""}

    print(f"ğŸ” Executing {len(search_queries)} parallel searches...")

    from concurrent.futures import ThreadPoolExecutor, as_completed

    search_results = []
    total_start_time = time.time()

    try:
        with ThreadPoolExecutor(max_workers=Config.MAX_WORKERS) as executor:
            # Submit all search tasks
            future_to_query = {
                executor.submit(
                    execute_single_search,
                    (i, query),
                    recent_search_mode,
                    search_days_limit,
                ): (i, query)
                for i, query in enumerate(search_queries)
            }

            # Collect results as they complete
            for future in as_completed(future_to_query):
                query_index, query = future_to_query[future]
                try:
                    result = future.result()
                    search_results.append(result)
                except Exception as exc:
                    print(f"âŒ Search {query_index + 1} generated exception: {exc}")
                    search_results.append(
                        {
                            "query": query,
                            "results": f"Exception: {str(exc)}",
                            "success": False,
                            "elapsed_time": 0,
                        }
                    )

    except Exception as e:
        print(f"âŒ Parallel search execution error: {e}")
        return {**state, "search_results": f"Parallel search error: {str(e)}"}

    total_elapsed_time = time.time() - total_start_time
    successful_searches = [r for r in search_results if r["success"]]
    failed_searches = [r for r in search_results if not r["success"]]

    print("ğŸ“Š Search Summary:")
    print(f"  âœ… Successful: {len(successful_searches)}/{len(search_queries)}")
    print(f"  âŒ Failed: {len(failed_searches)}")
    print(f"  â±ï¸ Total time: {total_elapsed_time:.2f}s")

    # If all searches failed, use WebSearch as fallback
    if len(successful_searches) == 0:
        print("ğŸ”„ All parallel searches failed - falling back to Claude Code WebSearch")

        try:
            websearch_results = execute_websearch_fallback(search_queries)

            print("âœ… WebSearch fallback completed")
            print(f"ğŸ“„ WebSearch results length: {len(websearch_results)} characters")

            # Create fallback results
            main_query = (
                search_queries[0] if search_queries else state.get("user_input", "")
            )
            combined_results = (
                "WebSearch Fallback Results (all parallel searches failed):\n\n"
            )
            combined_results += f"ğŸŒ WebSearch Query: {main_query}\n"
            combined_results += (
                f"â±ï¸ Fallback execution time: {total_elapsed_time:.2f}s\n"
            )
            combined_results += f"ğŸ“Š Results:\n{websearch_results}\n"
            combined_results += "-" * 50 + "\n\n"
            combined_results += "Original parallel search failures:\n"

            for i, result in enumerate(search_results, 1):
                combined_results += (
                    f"âŒ Search {i}: {result['query']} - {result['results']}\n"
                )

            return {
                **state,
                "search_results": combined_results,
                "parallel_search_stats": {
                    "total_queries": len(search_queries),
                    "successful": 0,
                    "failed": len(failed_searches),
                    "total_time": total_elapsed_time,
                    "websearch_fallback": True,
                },
            }

        except ImportError:
            print("âŒ Claude Code SDK not available for WebSearch fallback")
        except Exception as e:
            print(f"âŒ WebSearch fallback failed: {e}")

    # Format results using helper function
    combined_results = format_parallel_search_results(
        search_results, total_elapsed_time
    )

    return {
        **state,
        "search_results": combined_results,
        "parallel_search_stats": {
            "total_queries": len(search_queries),
            "successful": len(successful_searches),
            "failed": len(failed_searches),
            "total_time": total_elapsed_time,
        },
    }


def validate_slack_webhook_url(webhook_url: str) -> tuple[bool, str]:
    """Validate Slack webhook URL format."""
    if not webhook_url:
        return False, "SLACK_WEBHOOK_URL not found in environment variables"

    if not webhook_url.startswith("https://hooks.slack.com/"):
        return False, f"Invalid Slack webhook URL format: {webhook_url[:50]}..."

    return True, f"Slack webhook URL validated: {webhook_url[:30]}..."


def create_slack_payload(
    document_content: str, document_path: str, original_question: str
) -> Dict[str, any]:
    """Create Slack payload based on content size."""
    if len(document_content) > Config.SLACK_CONTENT_LIMIT:
        print(
            f"ğŸ“„ Large content detected ({len(document_content)} chars), using simplified format"
        )

        summary = f"""
ğŸ“„ LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œå®Œäº†

è³ªå•: {original_question}
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ‘ã‚¹: {document_path}

å†…å®¹ãŒå¤§ãã„ãŸã‚ã€å®Œå…¨ãªçµæœã¯ç”Ÿæˆã•ã‚ŒãŸãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚

å®Ÿè¡Œçµæœæ¦‚è¦:
- Ollamaã«ã‚ˆã‚‹åˆæœŸå›ç­”ç”Ÿæˆå®Œäº†
- Claude Codeã«ã‚ˆã‚‹ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨äº‹å®Ÿç¢ºèªå®Œäº†
- æœ€æ–°æƒ…å ±ã¨ã®ç…§åˆå®Œäº†
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆå®Œäº†
        """.strip()

        return {
            "text": summary,
            "username": "LangGraph Workflow Bot",
            "icon_emoji": ":memo:",
        }
    else:
        return {
            "text": f"""ğŸ“„ LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œå®Œäº†

è³ªå•: {original_question}
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç”Ÿæˆãƒ‘ã‚¹: `{document_path}`

çµæœ:
```
{document_content}
```""",
            "username": "LangGraph Workflow Bot",
            "icon_emoji": ":memo:",
        }


def send_slack_message_with_retry(
    webhook_url: str, payload: Dict[str, any], document_content: str
) -> bool:
    """Send Slack message with retry mechanism."""
    import requests
    import json

    retry_delay = Config.SLACK_INITIAL_RETRY_DELAY

    for attempt in range(Config.SLACK_MAX_RETRIES):
        try:
            print(f"ğŸ”„ é€ä¿¡è©¦è¡Œ {attempt + 1}/{Config.SLACK_MAX_RETRIES}")

            start_time = time.time()
            response = requests.post(
                webhook_url,
                data=json.dumps(payload),
                headers={"Content-Type": "application/json"},
                timeout=30,
            )
            response_time = time.time() - start_time

            if response.status_code == 200:
                print("âœ… Slack notification sent successfully")
                print(f"ğŸ“Š Document content size: {len(document_content)} characters")
                print(f"â±ï¸ Response time: {response_time:.2f} seconds")
                return True
            else:
                print(f"âŒ Slack notification failed: {response.status_code}")
                print(f"ğŸ“„ Response: {response.text}")

                # Don't retry for client errors
                if response.status_code in [400, 404]:
                    if response.status_code == 400:
                        print("ğŸ’¡ Bad Request - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ:")
                        print("  - Webhook URLãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„")
                    elif response.status_code == 404:
                        print("ğŸ’¡ Not Found - Webhook URLãŒç„¡åŠ¹ã¾ãŸã¯å‰Šé™¤ã•ã‚Œã¦ã„ã¾ã™")
                    return False

                if attempt < Config.SLACK_MAX_RETRIES - 1:
                    print(f"â³ {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                    time.sleep(retry_delay)
                    retry_delay *= 2  # Exponential backoff

        except (
            requests.exceptions.Timeout,
            requests.exceptions.ConnectionError,
            requests.exceptions.RequestException,
        ) as e:
            error_type = type(e).__name__
            print(
                f"ğŸ“¡ {error_type} (attempt {attempt + 1}/{Config.SLACK_MAX_RETRIES}): {e}"
            )

            if attempt < Config.SLACK_MAX_RETRIES - 1:
                print(f"â³ {retry_delay}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤ã—ã¾ã™...")
                time.sleep(retry_delay)
                retry_delay *= 2

    print(f"âŒ Slack notification failed after {Config.SLACK_MAX_RETRIES} attempts")
    return False


def slack_notification_node(state: WorkflowState) -> WorkflowState:
    """Send Slack notification with the complete document content and retry mechanism."""
    document_content = state.get("document_content", "")
    document_path = state.get("document_path", "")
    original_question = state.get("original_user_input", "")

    if not document_content:
        print("âš ï¸ No document content available for Slack notification")
        return {**state, "slack_notification_sent": False}

    print("ğŸ“¢ Sending Slack notification with document content...")

    try:
        # Get and validate Slack webhook URL
        slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")
        is_valid, validation_message = validate_slack_webhook_url(slack_webhook_url)

        if not is_valid:
            print(f"âš ï¸ {validation_message}")
            if "not found" in validation_message:
                print(
                    "ğŸ’¡ è¨­å®šæ–¹æ³•: export SLACK_WEBHOOK_URL='https://hooks.slack.com/your/webhook/url'"
                )
            elif "Invalid" in validation_message:
                print("ğŸ’¡ æ­£ã—ã„å½¢å¼: https://hooks.slack.com/services/...")
            return {**state, "slack_notification_sent": False}

        print(f"âœ… {validation_message}")

        # Create payload and send message
        slack_payload = create_slack_payload(
            document_content, document_path, original_question
        )
        success = send_slack_message_with_retry(
            slack_webhook_url, slack_payload, document_content
        )

        return {**state, "slack_notification_sent": success}

    except ImportError:
        print("âŒ requests library not available for Slack notification")
        print("ğŸ’¡ Install with: pip install requests")
        return {**state, "slack_notification_sent": False}
    except Exception as e:
        print(f"âŒ Unexpected error sending Slack notification: {e}")
        import traceback

        traceback.print_exc()
        return {**state, "slack_notification_sent": False}


def create_workflow() -> StateGraph:
    """Create and configure the LangGraph workflow with Ollama."""

    # Create the workflow graph
    workflow = StateGraph(WorkflowState)

    # Add nodes to the workflow
    workflow.add_node("input", input_node)
    workflow.add_node("query_generation", generate_search_queries)
    workflow.add_node("parallel_search", parallel_search_node)
    workflow.add_node("process", processing_node)
    workflow.add_node("review", review_node)
    workflow.add_node("document", documentation_node)
    # Check if Slack webhook URL is configured
    slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")
    if slack_webhook_url:
        workflow.add_node("slack_notification", slack_notification_node)

    # Define the workflow edges with conditional Slack notification
    workflow.add_edge(START, "input")
    workflow.add_edge("input", "query_generation")
    workflow.add_edge("query_generation", "parallel_search")
    workflow.add_edge("parallel_search", "process")
    workflow.add_edge("process", "review")
    workflow.add_edge("review", "document")

    if slack_webhook_url:
        workflow.add_edge("document", "slack_notification")
        workflow.add_edge("slack_notification", END)
    else:
        workflow.add_edge("document", END)

    return workflow


def check_ollama_connection() -> bool:
    """Check if Ollama is running and the configured model is available."""
    try:
        import requests

        print("ğŸ” Checking Ollama connection...")

        response = requests.get(f"{Config.OLLAMA_BASE_URL}/api/tags", timeout=5)

        if response.status_code != 200:
            print(f"âŒ Ollama API returned error: {response.status_code}")
            return False

        models = response.json()
        model_names = [model["name"] for model in models.get("models", [])]

        print(f"âœ… Ollama is running with {len(model_names)} models")

        if Config.OLLAMA_MODEL in model_names:
            print(f"âœ… {Config.OLLAMA_MODEL} model is available")
            return True
        else:
            print(f"âŒ {Config.OLLAMA_MODEL} model not found")
            print("Available models:", model_names)
            print(
                f"\nğŸ’¡ To install {Config.OLLAMA_MODEL}, run: ollama pull {Config.OLLAMA_MODEL}"
            )
            return False

    except requests.exceptions.RequestException as e:
        print(f"âŒ Cannot connect to Ollama: {e}")
        print("\nğŸ’¡ Make sure Ollama is running: ollama serve")
        return False
    except ImportError:
        print("âŒ requests library not available for Ollama check")
        return False


def main():
    """Main function to run the workflow with Ollama."""
    print("ğŸš€ Starting LangGraph Workflow with Ollama gpt-oss:20b")
    print("=" * 60)

    # Check Ollama connection
    ollama_available = check_ollama_connection()
    if not ollama_available:
        print("\nâš ï¸  Continuing anyway - will use fallback responses if needed")

    print()

    # Get user input
    print("ğŸ’¬ Please enter your question:")
    try:
        user_question = input("â“ ")
    except EOFError:
        user_question = ""

    if not user_question.strip():
        user_question = "Explain the concept of LangGraph workflows and their benefits for AI applications"
        print(f"ğŸ”„ Using default question: {user_question}")

    # Create the workflow
    workflow = create_workflow()

    # Compile the workflow
    app = workflow.compile()

    # Initial state
    initial_state = {
        "messages": [],
        "iteration": 0,
        "user_input": user_question,
        "original_user_input": user_question,  # Store original question
        "processed_output": "",
        "should_continue": True,
        "search_results": "",
        "search_queries": [],  # Store generated search queries
        "parallel_search_stats": {},  # Store parallel search statistics
        "recent_search_mode": False,
        "search_days_limit": 60,  # Default to 2 months
        "initial_output": "",  # Store first AI output for comparison
        "reviewed_output": "",  # Store Claude Code reviewed output
        "document_generated": False,  # Track document generation status
        "document_content": "",  # Store generated markdown content
        "document_path": "",  # Store path to generated document
        "slack_notification_sent": not bool(
            os.getenv("SLACK_WEBHOOK_URL")
        ),  # Track Slack notification status (True if not needed)
    }

    print("\nğŸ“‹ Initial State:")
    print(f"  User Input: {initial_state['user_input']}")
    print(f"  Iteration: {initial_state['iteration']}")
    print()

    # Execute the workflow
    print("âš¡ Executing Workflow with Ollama...")
    print("-" * 40)

    try:
        final_state = app.invoke(initial_state)

        print("\nâœ… Workflow Completed!")
        print("=" * 60)
        print("ğŸ“Š Final Results:")
        print(f"  Total Iterations: {final_state['iteration']}")
        print(f"  Message Count: {len(final_state['messages'])}")
        print(
            f"  Document Generated: {'âœ…' if final_state.get('document_generated', False) else 'âŒ'}"
        )
        print(
            f"  Slack Notification: {'âœ…' if final_state.get('slack_notification_sent', False) else 'âŒ'}"
        )
        print()

        print("ğŸ’¬ Full Conversation History:")
        for i, message in enumerate(final_state["messages"], 1):
            message_type = (
                "User" if isinstance(message, HumanMessage) else "AI (gpt-oss:20b)"
            )
            content = message.content
            print(f"  {i}. [{message_type}]:")
            print("-" * 50)
            print(content)
            print("-" * 50)
            print()

        # Display review results if available
        if final_state.get("reviewed_output"):
            print("ğŸ” Claude Code Review Results:")
            print("=" * 60)
            print(final_state["reviewed_output"])
            print("=" * 60)
            print()

        # Display documentation status
        if final_state.get("document_generated"):
            print("ğŸ“ Documentation successfully generated in Docs/ directory")
            print(f"ğŸ“„ Document path: {final_state.get('document_path', 'Unknown')}")
        else:
            print("âš ï¸ Documentation generation failed or was skipped")

        # Display Slack notification status
        slack_webhook_url = os.getenv("SLACK_WEBHOOK_URL")
        if slack_webhook_url:
            if final_state.get("slack_notification_sent"):
                print(
                    "ğŸ“¢ Slack notification sent successfully with complete document content"
                )
            else:
                print(
                    "âš ï¸ Slack notification failed (check SLACK_WEBHOOK_URL environment variable)"
                )
        else:
            print("â„¹ï¸ Slack notification skipped (SLACK_WEBHOOK_URL not configured)")

    except Exception as e:
        print(f"âŒ Workflow execution failed: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
