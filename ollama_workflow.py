#!/usr/bin/env python3
"""LangGraph workflow implementation with Ollama gpt-oss:20b model."""

from dotenv import load_dotenv
from langgraph.graph import StateGraph, START, END
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_ollama import ChatOllama
from typing_extensions import TypedDict
import asyncio
import os
import datetime
from pathlib import Path

# Load environment variables
load_dotenv()


# Define the state structure for our workflow
class WorkflowState(TypedDict):
    messages: list[BaseMessage]
    iteration: int
    user_input: str
    original_user_input: str  # Store original question for iterations
    processed_output: str
    should_continue: bool
    search_results: str
    recent_search_mode: bool
    initial_output: str  # Store first AI output for comparison
    reviewed_output: str  # Store Claude Code reviewed output
    document_generated: bool  # Track document generation status


def search_node(state: WorkflowState) -> WorkflowState:
    """Search for relevant information using psearch with real-time output."""
    user_input = state.get("user_input", "")
    recent_search_mode = state.get("recent_search_mode", False)

    if not user_input:
        return {**state, "search_results": ""}

    search_mode_text = " (limited to past 2 months)" if recent_search_mode else ""
    print(f"ğŸ” Searching for information about: {user_input}{search_mode_text}")
    print("ğŸ“Š Progress visualization:")
    print("-" * 40)

    try:
        import subprocess
        import sys
        import time

        # Use psearch to search for relevant information
        # Format the query for better search results
        search_query = user_input[:100]  # Limit query length

        # Record start time
        start_time = time.time()

        # Build psearch command with optional date filtering
        psearch_cmd = ["psearch", "search", search_query, "-n", "5", "-c", "--json"]

        # Add date filter for recent search mode (past 2 months)
        if recent_search_mode:
            psearch_cmd.extend(["-r", "--months", "2", "-s"])
            print("ğŸ“… Filtering results: recent only (past 2 months), sorted by date")

        # Run psearch command with real-time output streaming
        process = subprocess.Popen(
            psearch_cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,  # Line buffering
            universal_newlines=True,
        )

        # Collect output while displaying progress
        stdout_lines = []
        stderr_lines = []

        # Read stdout line by line for real-time display
        while True:
            output = process.stdout.readline()
            if output == "" and process.poll() is not None:
                break
            if output:
                # Display the line immediately for progress visualization
                print(f"ğŸ“¤ {output.rstrip()}")
                sys.stdout.flush()  # Force immediate output
                stdout_lines.append(output)

        # Get any remaining stderr
        stderr_output = process.stderr.read()
        if stderr_output:
            stderr_lines.append(stderr_output)

        # Wait for process to complete
        return_code = process.wait()

        # Calculate elapsed time
        elapsed_time = time.time() - start_time

        print("-" * 40)
        print(f"â±ï¸ Search completed in {elapsed_time:.2f} seconds")

        if return_code == 0:
            search_output = "".join(stdout_lines)
            print("âœ… Search completed successfully")
            print(
                f"ğŸ“„ Found {len(search_output.split('---')) - 1 if '---' in search_output else 'some'} results"
            )

            # Summarize search results for LLM processing
            search_summary = (
                f"Search results for '{user_input}':\n\n{search_output[:2000]}..."
            )

            return {
                **state,
                "search_results": search_summary,
            }
        else:
            stderr_output = "".join(stderr_lines)
            print(f"âš ï¸ Search failed with return code {return_code}")
            print(f"Error: {stderr_output}")
            return {
                **state,
                "search_results": f"Search failed: {stderr_output}",
            }

    except FileNotFoundError:
        print("âŒ psearch command not found")
        return {
            **state,
            "search_results": "psearch command not available",
        }
    except Exception as e:
        print(f"âŒ Search error: {e}")
        return {
            **state,
            "search_results": f"Search error: {str(e)}",
        }


def input_node(state: WorkflowState) -> WorkflowState:
    """Process initial user input and detect recent search keywords."""
    user_input = state.get("user_input", "")
    messages = state.get("messages", [])

    # Add user message to conversation
    if user_input:
        messages.append(HumanMessage(content=user_input))

    # Check for keywords that indicate user wants recent information
    recent_keywords = [
        "æœ€æ–°",
        "ç›´è¿‘",
        "æœ€è¿‘",
        "æ–°ã—ã„",
        "ä»Šæ—¥",
        "ä»Šé€±",
        "ä»Šæœˆ",
        "latest",
        "recent",
        "new",
        "current",
        "today",
        "this week",
        "this month",
        "2024å¹´",
        "2025å¹´",
        "ä»Šå¹´",
        "this year",
    ]

    recent_search_mode = any(keyword in user_input for keyword in recent_keywords)

    if recent_search_mode:
        print(
            "ğŸ” Recent information keywords detected - search will be limited to past 2 months"
        )

    return {
        **state,
        "messages": messages,
        "iteration": state.get("iteration", 0) + 1,
        "recent_search_mode": recent_search_mode,
        "original_user_input": state.get(
            "original_user_input", user_input
        ),  # Store original on first iteration
    }


def processing_node(state: WorkflowState) -> WorkflowState:
    """Process the user input using Ollama gpt-oss:20b model with search results."""
    messages = state["messages"]
    iteration = state["iteration"]
    search_results = state.get("search_results", "")

    if not messages:
        return state

    print(f"ğŸ¤– Processing iteration {iteration} with Ollama gpt-oss:20b...")

    try:
        # Initialize Ollama with gpt-oss:20b model
        llm = ChatOllama(
            model="gpt-oss:20b",
            base_url="http://localhost:11434",  # Default Ollama port
            temperature=0.7,
        )

        # Create a focused prompt for the LLM
        last_message = messages[-1]
        if isinstance(last_message, HumanMessage):
            content = last_message.content

            # Create a system prompt that includes search results
            system_prompt = f"""
            ã‚ãªãŸã¯LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®{iteration}å›ç›®ã®å‡¦ç†ã‚’è¡Œã†AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
            æœ€æ–°ã®æ¤œç´¢çµæœã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ã€æ­£ç¢ºã§æœ€æ–°ã®æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã«å¯¾ã—ã¦ã€å¿…è¦ã«å¿œã˜ã¦æ¤œç´¢çµæœã‹ã‚‰é–¢é€£æƒ…å ±ã‚’å–ã‚Šå…¥ã‚ŒãŸã€æ€æ…®æ·±ã„å›ç­”ã‚’æ—¥æœ¬èªã§æä¾›ã—ã¦ãã ã•ã„ã€‚
            ç°¡æ½”ã§ã‚ã‚ŠãªãŒã‚‰ã€æƒ…å ±é‡è±Šå¯Œãªå›ç­”ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚
            
            ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›: {content}
            
            æ¤œç´¢çµæœ (åˆ©ç”¨å¯èƒ½ãªå ´åˆ):
            {search_results if search_results else "æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“"}
            
            æ¤œç´¢çµæœã‚’æ´»ç”¨ã—ã¦ã€æœ€æ–°ã§æ­£ç¢ºãªæƒ…å ±ã‚’å«ã‚ãŸæ—¥æœ¬èªã§ã®å›ç­”ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
            """

            # Get response from Ollama
            response = llm.invoke([HumanMessage(content=system_prompt)])

            # Add AI response to messages
            ai_response = response.content
            messages.append(AIMessage(content=ai_response))

            print("âœ… LLM Full Response:")
            print("-" * 60)
            print(ai_response)
            print("-" * 60)

            # Store initial output for comparison (first iteration only)
            if state.get("iteration", 0) == 1:
                return {
                    **state,
                    "messages": messages,
                    "processed_output": ai_response,
                    "initial_output": ai_response,
                }
            else:
                return {
                    **state,
                    "messages": messages,
                    "processed_output": ai_response,
                }

    except Exception as e:
        print(f"âŒ Error calling Ollama: {e}")
        print("ğŸ”„ Falling back to simple response generation...")

        # Fallback to simple processing if Ollama is not available
        if messages and isinstance(messages[-1], HumanMessage):
            content = messages[-1].content
            fallback_response = (
                f"Processing iteration {iteration}: {content} (Ollama unavailable)"
            )
            messages.append(AIMessage(content=fallback_response))

            return {
                **state,
                "messages": messages,
                "processed_output": fallback_response,
            }

    return state


def decision_node(state: WorkflowState) -> WorkflowState:
    """Decide whether to continue processing or end."""
    iteration = state.get("iteration", 0)

    # Continue for fewer iterations when using LLM (to avoid long execution)
    should_continue = iteration < 2

    return {
        **state,
        "should_continue": should_continue,
    }


def continuation_node(state: WorkflowState) -> WorkflowState:
    """Continue processing with original context and previous response preserved."""
    messages = state["messages"]
    iteration = state["iteration"]
    original_question = state.get("original_user_input", "")
    previous_output = state.get("processed_output", "")

    # Create a context-aware continuation message that includes previous response
    continuation_message = f"""å…ƒã®è³ªå•ã€Œ{original_question}ã€ã«ã¤ã„ã¦ã€ä»¥ä¸‹ãŒå‰å›ã®å›ç­”ã§ã™ï¼š

ã€å‰å›ã®å›ç­”ã€‘
{previous_output}

ã“ã®å‰å›ã®å›ç­”ã‚’è¸ã¾ãˆã¦ã€ä»¥ä¸‹ã®ç‚¹ã§æ›´ã«è©³ã—ãæ˜ã‚Šä¸‹ã’ã¦èª¬æ˜ã—ã¦ãã ã•ã„ï¼š

â€¢ ã‚ˆã‚Šå…·ä½“çš„ãªå®Ÿè·µä¾‹ã‚„äº‹ä¾‹
â€¢ è©³ç´°ãªæ‰‹é †ã‚„ãƒ—ãƒ­ã‚»ã‚¹
â€¢ é–¢é€£ã™ã‚‹æœ€æ–°ã®å‹•å‘ã‚„ç™ºå±•
â€¢ æ³¨æ„ç‚¹ã€èª²é¡Œã€è€ƒæ…®äº‹é …
â€¢ å®Ÿè£…æ™‚ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

ã“ã‚Œã¯{iteration}å›ç›®ã®è©³ç´°åŒ–ã§ã™ã€‚å‰å›ã®å†…å®¹ã¨é‡è¤‡ã—ãªã„ã‚ˆã†ã€æ–°ã—ã„è§’åº¦ã‹ã‚‰ã®æƒ…å ±ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"""

    return {
        **state,
        "messages": messages,
        "user_input": continuation_message,
    }


def review_node(state: WorkflowState) -> WorkflowState:
    """Use Claude Code SDK to review and correct the final output."""
    processed_output = state.get("processed_output", "")
    original_question = state.get("original_user_input", "")
    
    if not processed_output:
        print("âš ï¸ No output to review")
        return {**state, "reviewed_output": ""}
    
    print("ğŸ” Reviewing output with Claude Code SDK...")
    
    try:
        # Try importing Claude Code SDK
        from claude_code_sdk import query, ClaudeCodeOptions
        
        # Create review prompt
        review_prompt = f"""
ä»¥ä¸‹ã¯ã€Œ{original_question}ã€ã¨ã„ã†è³ªå•ã«å¯¾ã™ã‚‹AIã®å›ç­”ã§ã™ã€‚

ã€å¯¾è±¡ã®å›ç­”ã€‘
{processed_output}

ã“ã®å›ç­”ã‚’è©³ç´°ã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ä»¥ä¸‹ã®ç‚¹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ä¿®æ­£ç‰ˆã‚’æä¾›ã—ã¦ãã ã•ã„ï¼š

1. äº‹å®Ÿã®æ­£ç¢ºæ€§ï¼ˆæŠ€è¡“çš„ãªé–“é•ã„ã‚„å¤ã„æƒ…å ±ãŒãªã„ã‹ï¼‰
2. è«–ç†çš„ãªä¸€è²«æ€§ï¼ˆçŸ›ç›¾ã™ã‚‹å†…å®¹ãŒãªã„ã‹ï¼‰
3. å®Œå…¨æ€§ï¼ˆé‡è¦ãªæƒ…å ±ãŒæŠœã‘ã¦ã„ãªã„ã‹ï¼‰
4. ã‚ã‹ã‚Šã‚„ã™ã•ï¼ˆèª¬æ˜ãŒæ˜ç¢ºã§ç†è§£ã—ã‚„ã™ã„ã‹ï¼‰
5. æœ€æ–°æ€§ï¼ˆæœ€æ–°ã®æƒ…å ±ã«åŸºã¥ã„ã¦ã„ã‚‹ã‹ï¼‰

ã‚‚ã—é–“é•ã„ã‚„æ”¹å–„ç‚¹ãŒã‚ã‚Œã°ã€ä¿®æ­£ã•ã‚ŒãŸå†…å®¹ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
å•é¡ŒãŒãªã„å ´åˆã¯ã€Œãƒ¬ãƒ“ãƒ¥ãƒ¼å®Œäº†ï¼šå•é¡Œãªã—ã€ã¨å›ç­”ã—ã¦ãã ã•ã„ã€‚

ä¿®æ­£ç‰ˆãŒã‚ã‚Œã°æ—¥æœ¬èªã§æä¾›ã—ã€ä¿®æ­£ç‚¹ã‚‚ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
"""

        # Configure options for Claude Code
        options = ClaudeCodeOptions(
            system_prompt="ã‚ãªãŸã¯æŠ€è¡“æ–‡æ›¸ã®æ ¡æ­£ãƒ»ãƒ¬ãƒ“ãƒ¥ãƒ¼ã®å°‚é–€å®¶ã§ã™ã€‚æ­£ç¢ºæ€§ã¨æœ€æ–°æ€§ã‚’é‡è¦–ã—ã¦ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚",
            max_turns=1,
            allowed_tools=["WebSearch"]  # Allow web search for fact checking
        )
        
        reviewed_content = ""
        
        # Query Claude Code SDK using asyncio
        async def get_review():
            content = ""
            async for message in query(prompt=review_prompt, options=options):
                if hasattr(message, 'content'):
                    if isinstance(message.content, list):
                        for block in message.content:
                            if hasattr(block, 'text'):
                                content += block.text
                    else:
                        content += str(message.content)
            return content
        
        # Run async function
        import asyncio
        try:
            reviewed_content = asyncio.run(get_review())
        except Exception as async_error:
            print(f"âŒ Async execution error: {async_error}")
            reviewed_content = f"éåŒæœŸå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {async_error}\n\nå…ƒã®å›ç­”:\n{processed_output}"
        
        print("âœ… Review completed with Claude Code SDK")
        print("-" * 60)
        print(reviewed_content)
        print("-" * 60)
        
        return {
            **state,
            "reviewed_output": reviewed_content,
        }
        
    except ImportError:
        print("âŒ Claude Code SDK not available, skipping review")
        return {
            **state,
            "reviewed_output": f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸï¼ˆClaude Code SDKåˆ©ç”¨ä¸å¯ï¼‰\n\nå…ƒã®å›ç­”:\n{processed_output}",
        }
    except Exception as e:
        print(f"âŒ Error during review: {e}")
        return {
            **state,
            "reviewed_output": f"ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\n\nå…ƒã®å›ç­”:\n{processed_output}",
        }


def documentation_node(state: WorkflowState) -> WorkflowState:
    """Generate markdown documentation comparing initial and final outputs."""
    original_question = state.get("original_user_input", "")
    initial_output = state.get("initial_output", "")
    reviewed_output = state.get("reviewed_output", "")
    search_results = state.get("search_results", "")
    
    print("ğŸ“ Generating documentation...")
    
    try:
        # Create docs directory if it doesn't exist
        docs_dir = Path.home() / "workspace" / "Docs"
        docs_dir.mkdir(parents=True, exist_ok=True)
        
        # Create a descriptive title from the original question
        question_summary = original_question[:30].replace("/", "").replace("\\", "").replace(":", "ï¼š").replace("?", "ï¼Ÿ").replace("*", "").replace("<", "").replace(">", "").replace("|", "")
        if len(original_question) > 30:
            question_summary += "..."
        
        filename = f"{question_summary}_åˆ†æçµæœ.md"
        file_path = docs_dir / filename
        
        # Generate markdown content
        markdown_content = f"""# LangGraphãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼å®Ÿè¡Œçµæœ

## å®Ÿè¡Œæƒ…å ±
- **å®Ÿè¡Œæ—¥æ™‚**: {datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S")}
- **è³ªå•**: {original_question}
- **ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**: {state.get("iteration", 0)}

## å…ƒã®è³ªå•
```
{original_question}
```

## æ¤œç´¢çµæœã®æ¦‚è¦
```
{search_results[:500] if search_results else "æ¤œç´¢çµæœãªã—"}...
```

## åˆå›AIå›ç­”ï¼ˆOllama gpt-oss:20bï¼‰
{initial_output if initial_output else "åˆå›å›ç­”ãªã—"}

## Claude Codeãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœ
{reviewed_output if reviewed_output else "ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœãªã—"}

## æ¯”è¼ƒåˆ†æ
### æ”¹å–„ç‚¹
- Claude Codeã«ã‚ˆã‚‹äº‹å®Ÿç¢ºèªã¨ä¿®æ­£
- ã‚ˆã‚Šæ­£ç¢ºã§æœ€æ–°ã®æƒ…å ±ã®æä¾›
- è«–ç†çš„ä¸€è²«æ€§ã®å‘ä¸Š

### å­¦ç¿’ãƒã‚¤ãƒ³ãƒˆ
- è¤‡æ•°ã®AIã‚·ã‚¹ãƒ†ãƒ ã‚’é€£æºã•ã›ã‚‹ã“ã¨ã§å›ç­”å“è³ªãŒå‘ä¸Š
- å¤–éƒ¨æ¤œç´¢ã¨ã®çµ„ã¿åˆã‚ã›ã§æœ€æ–°æƒ…å ±ã‚’å–å¾—
- ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã«ã‚ˆã‚Šä¿¡é ¼æ€§ãŒå‘ä¸Š

---
*ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ LangGraph + Claude Code SDK ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã«ã‚ˆã‚Šè‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""
        
        # Write to file
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(markdown_content)
        
        print(f"âœ… Documentation generated: {file_path}")
        
        return {
            **state,
            "document_generated": True,
        }
        
    except Exception as e:
        print(f"âŒ Error generating documentation: {e}")
        return {
            **state,
            "document_generated": False,
        }


def create_workflow() -> StateGraph:
    """Create and configure the LangGraph workflow with Ollama."""

    # Create the workflow graph
    workflow = StateGraph(WorkflowState)

    # Add nodes to the workflow
    workflow.add_node("input", input_node)
    workflow.add_node("search", search_node)
    workflow.add_node("process", processing_node)
    workflow.add_node("decision", decision_node)
    workflow.add_node("continue", continuation_node)
    workflow.add_node("review", review_node)
    workflow.add_node("document", documentation_node)

    # Define the workflow edges
    workflow.add_edge(START, "input")
    workflow.add_edge("input", "search")
    workflow.add_edge("search", "process")
    workflow.add_edge("process", "decision")

    # Conditional routing function
    def route_decision(state: WorkflowState) -> str:
        """Route based on the decision state."""
        return "continue" if state.get("should_continue", False) else "review"

    # Conditional edges from decision node
    workflow.add_conditional_edges(
        "decision",
        route_decision,
        {
            "continue": "continue",
            "review": "review",
        },
    )

    # Edge from continue back to input for loop
    workflow.add_edge("continue", "input")
    
    # New edges for review and documentation
    workflow.add_edge("review", "document")
    workflow.add_edge("document", END)

    return workflow


def check_ollama_connection():
    """Check if Ollama is running and gpt-oss:20b is available."""
    try:
        import requests

        print("ğŸ” Checking Ollama connection...")

        # Check if Ollama is running
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            models = response.json()
            model_names = [model["name"] for model in models.get("models", [])]

            print(f"âœ… Ollama is running with {len(model_names)} models")

            if "gpt-oss:20b" in model_names:
                print("âœ… gpt-oss:20b model is available")
                return True
            else:
                print("âŒ gpt-oss:20b model not found")
                print("Available models:", model_names)
                print("\nğŸ’¡ To install gpt-oss:20b, run: ollama pull gpt-oss:20b")
                return False
        else:
            print("âŒ Ollama API returned error:", response.status_code)
            return False

    except requests.exceptions.RequestException as e:
        print("âŒ Cannot connect to Ollama:", e)
        print("\nğŸ’¡ Make sure Ollama is running: ollama serve")
        return False
    except ImportError:
        print("âŒ requests library not available for Ollama check")
        return False


def main():
    """Main function to run the workflow with Ollama."""
    print("ğŸš€ Starting LangGraph Workflow with Ollama gpt-oss:20b")
    print("=" * 60)

    # Check Ollama connection
    ollama_available = check_ollama_connection()
    if not ollama_available:
        print("\nâš ï¸  Continuing anyway - will use fallback responses if needed")

    print()

    # Get user input
    print("ğŸ’¬ Please enter your question:")
    user_question = input("â“ ")

    if not user_question.strip():
        user_question = "Explain the concept of LangGraph workflows and their benefits for AI applications"
        print(f"ğŸ”„ Using default question: {user_question}")

    # Create the workflow
    workflow = create_workflow()

    # Compile the workflow
    app = workflow.compile()

    # Initial state
    initial_state = {
        "messages": [],
        "iteration": 0,
        "user_input": user_question,
        "original_user_input": user_question,  # Store original question
        "processed_output": "",
        "should_continue": True,
        "search_results": "",
        "recent_search_mode": False,
        "initial_output": "",  # Store first AI output for comparison
        "reviewed_output": "",  # Store Claude Code reviewed output
        "document_generated": False,  # Track document generation status
    }

    print("\nğŸ“‹ Initial State:")
    print(f"  User Input: {initial_state['user_input']}")
    print(f"  Iteration: {initial_state['iteration']}")
    print()

    # Execute the workflow
    print("âš¡ Executing Workflow with Ollama...")
    print("-" * 40)

    try:
        final_state = app.invoke(initial_state)

        print("\nâœ… Workflow Completed!")
        print("=" * 60)
        print("ğŸ“Š Final Results:")
        print(f"  Total Iterations: {final_state['iteration']}")
        print(f"  Message Count: {len(final_state['messages'])}")
        print(f"  Document Generated: {'âœ…' if final_state.get('document_generated', False) else 'âŒ'}")
        print()

        print("ğŸ’¬ Full Conversation History:")
        for i, message in enumerate(final_state["messages"], 1):
            message_type = (
                "User" if isinstance(message, HumanMessage) else "AI (gpt-oss:20b)"
            )
            content = message.content
            print(f"  {i}. [{message_type}]:")
            print("-" * 50)
            print(content)
            print("-" * 50)
            print()

        # Display review results if available
        if final_state.get("reviewed_output"):
            print("ğŸ” Claude Code Review Results:")
            print("=" * 60)
            print(final_state["reviewed_output"])
            print("=" * 60)
            print()
        
        # Display documentation status
        if final_state.get("document_generated"):
            print("ğŸ“ Documentation successfully generated in Docs/ directory")
        else:
            print("âš ï¸ Documentation generation failed or was skipped")

    except Exception as e:
        print(f"âŒ Workflow execution failed: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
